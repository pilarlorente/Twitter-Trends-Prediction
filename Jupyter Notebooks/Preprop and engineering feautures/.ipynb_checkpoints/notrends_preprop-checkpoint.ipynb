{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_dict(string):\n",
    "    \"\"\"Transforma una cadena de caracteres con forma de diccionario a diccionario\"\"\"\n",
    "    if string != \"[]\":\n",
    "        string = json.loads(string.replace(\"'\", \"\\\"\"))\n",
    "        return \",\".join([s[\"screen_name\"] for s in string])\n",
    "    return \"\"\n",
    "\n",
    "def to_list(list_):\n",
    "    \"\"\"Transforma una cadena de caracteres con forma de lista a lista\"\"\"\n",
    "    if list_ != \"[]\":\n",
    "        list_ = list_[1:-1]\n",
    "        list_ = list_.split(\",\")\n",
    "        return \",\".join([s.strip().strip(\"'\") for s in list_])\n",
    "    return \"\"\n",
    "\n",
    "def normalize(s):\n",
    "    \"\"\"Reemplaza las letras con tildes y retorna la cadena de caracteres en minuscula\"\"\"\n",
    "    replacements = ((\"á\", \"a\"), (\"é\", \"e\"), (\"í\", \"i\"), (\"ó\", \"o\"), (\"ú\", \"u\"))\n",
    "    for a, b in replacements:\n",
    "        s = s.lower()\n",
    "        s = s.replace(a, b)\n",
    "    return s\n",
    "\n",
    "def deEmojify(text):\n",
    "    \"\"\"Quita los emojis de los tweets\"\"\"\n",
    "    regrex_pattern = re.compile(pattern = \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags = re.UNICODE)\n",
    "    return regrex_pattern.sub(r\"\", text)\n",
    "\n",
    "def cleanTxt(text):\n",
    "    \"\"\"Elimina mentions, hiperlinks, quita el simbolo \"#\" y el \"RT\"\"\"\"\n",
    "    text = re.sub(r\"@[a-zA-Z0-9]+\", \"\", text) #Removes @mentions\n",
    "    text = re.sub(r\"#\", \"\", text) #Removing the \"#\" symbol\n",
    "    text = re.sub(r\"RT[\\s]+\", \"\", text) #Removing RT\n",
    "    text = re.sub(r\"https?:\\/\\/\\S+\", \"\", text) #Remove the hyperlink\n",
    "    return text\n",
    "\n",
    "def replace_punct(s):\n",
    "    \"\"\"Elimina los signos de puntuacion\"\"\"\n",
    "    for i in string.punctuation:\n",
    "        if i in s:\n",
    "            s = s.replace(i, \"\").strip()\n",
    "    return s\n",
    "\n",
    "def replace_num(s):\n",
    "    \"\"\"Remueve los numeros de los tweets\"\"\"\n",
    "    for i in [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"]:\n",
    "        s = s.replace(i, \"\")\n",
    "    return s\n",
    "\n",
    "def tokenizador(text):\n",
    "    \"\"\"Tokeniza el texto del tweet\"\"\"\n",
    "    important_words = []\n",
    "    for word in text.split(\" \"):\n",
    "        if word not in stopwords.words(\"spanish\"):\n",
    "            if word != \"\":\n",
    "                important_words.append(word)\n",
    "    return \" \".join(important_words).strip()\n",
    "\n",
    "def foo(text):\n",
    "    \"\"\"Elimina mas signos de puntuacion\"\"\"\n",
    "    forbidden = (\"?\", \"¿\", \"¡\", \"!\", \",\", \".\", \";\", \":\", \"-\", \"'\", \"+\", \"$\", \"/\", \"*\",'«','»', \"~\", \"(\", \")\")\n",
    "    aux = \"\"\n",
    "    for v in text:\n",
    "        if not v in forbidden:\n",
    "            aux += v\n",
    "    return aux\n",
    "\n",
    "def quita_palabras_pequeñas(text):\n",
    "    \"\"\"Quita palabras de longitud menor a 4 del texto del tweet\"\"\"\n",
    "    return \" \".join([word for word in text.split(\" \") if len(word) >= 5])            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df = pd.read_csv('C:/Users/Daniel/Desktop/csv/dia 25/no trends/tweets_25_notendencias_raw.csv')\n",
    "df.drop(['Unnamed: 0','Unnamed: 0.1'], axis = 1, inplace = True)\n",
    "\n",
    "df_summary = pd.read_csv(\"C:/Users/Daniel/Desktop/por hora/lista_tendencias_25_por_hora.csv\", sep = \";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPROCESAMIENTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1. Hace drop a las columnas de ids, husos horarios, url y traducciones\n",
    "# 2. Filtra los tweets por idioma (\"es\")\n",
    "\n",
    "columns_to_drop = [\"conversation_id\", \"cashtags\", \"timezone\", \"user_id\", \"name\", \"near\", \"geo\", \"source\",\n",
    "                   \"user_rt_id\", \"user_rt\", \"retweet_id\", \"retweet_date\", \"translate\", \"trans_src\",\n",
    "                   \"trans_dest\", \"place\", \"quote_url\", \"thumbnail\", \"created_at\", \"id\", \"link\"]\n",
    "\n",
    "df.drop(columns_to_drop, axis = 1, inplace = True)\n",
    "\n",
    "df = df[df.language == \"es\"]\n",
    "\n",
    "df.drop(\"language\", axis = 1, inplace = True)\n",
    "\n",
    "df = df.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforma la columna \"reply_to\" a diccionario\n",
    "# Elimina las filas donde no es posible\n",
    "\n",
    "reply_to_rows = []\n",
    "for num, row in enumerate(df.reply_to):\n",
    "    try:\n",
    "        to_dict(row)\n",
    "    except:\n",
    "        reply_to_rows.append(num)\n",
    "        \n",
    "df.drop(reply_to_rows, inplace = True)\n",
    "\n",
    "df.reply_to = df.reply_to.apply(to_dict)\n",
    "\n",
    "df = df.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforma la columna \"mentions\" a diccionario\n",
    "# Elimina las filas donde no es posible\n",
    "\n",
    "mention_rows = []\n",
    "for num, row in enumerate(df.mentions):\n",
    "    try:\n",
    "        to_dict(row)\n",
    "    except:\n",
    "        mention_rows.append(num)\n",
    "        \n",
    "df.drop(mention_rows, inplace = True)\n",
    "\n",
    "df.mentions = df.mentions.apply(to_dict)\n",
    "\n",
    "df = df.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforma la columna \"hashtags\" a lista\n",
    "# Elimina las filas donde no es posible\n",
    "\n",
    "hashtags_rows = []\n",
    "for num, row in enumerate(df.hashtags):\n",
    "    try:\n",
    "        to_list(row)\n",
    "    except:\n",
    "        hashtags_rows.append(num)\n",
    "        \n",
    "df.drop(hashtags_rows, inplace = True)\n",
    "\n",
    "df.hashtags = df.hashtags.apply(to_list)\n",
    "\n",
    "df = df.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A las columnas \"photos\", \"retweet\" y \"url\" las cambiamos por valores de 0 y 1\n",
    "# 0 si no hay photo, url o si el tweet no es retweet\n",
    "# 1 si hay photo, url o si el tweet es retweet\n",
    "\n",
    "df.photos = df.photos.apply(lambda x : 1 if x != \"[]\" else 0)\n",
    "df.retweet = df.retweet.apply(lambda x : 1 if x == \"True\" else 0)\n",
    "df.urls = df.urls.apply(lambda x : 1 if x != \"[]\" else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columnas de tiempo\n",
    "\n",
    "df[\"month\"] = df.date.apply(lambda x : int(x[5 : 7]))\n",
    "df[\"day\"] = df.date.apply(lambda x : int(x[-2:]))\n",
    "\n",
    "df[\"hour\"] = df.time.apply(lambda x : int(x[:2]))\n",
    "df[\"minute\"] = df.time.apply(lambda x : int(x[3:5]))\n",
    "df[\"second\"] = df.time.apply(lambda x : int(x[6:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columnas de interaccion:\n",
    "# \"mentions_count\" : cuenta cuantas mentions hay en el tweet\n",
    "# \"reply_to_count\" : cuenta a cuantas personas le hace respuesta el tweet\n",
    "# \"hashtags_count\" : cuenta cuantos hashtags hay en el tweet\n",
    "\n",
    "# \"interaccion\" : es la summa de las 3 columnas anteriores\n",
    "\n",
    "df[\"mentions_count\"] = [len(mention.split(\",\")) if type(mention) == str else 0 for mention in df.mentions]\n",
    "df[\"reply_to_count\"] = [len(reply.split(\",\"))   if type(reply)   == str else 0 for reply   in df.reply_to]\n",
    "df[\"hashtags_count\"] = [len(hashtag.split(\",\")) if type(hashtag) == str else 0 for hashtag in df.hashtags]\n",
    "\n",
    "df[\"interaccion\"] = [rt + re + lk for rt, re, lk in zip(df.retweets_count, df.replies_count, df.likes_count)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elimina las filas donde la fecha es NaN\n",
    "\n",
    "indices_todrop = list()\n",
    "for num, time in enumerate(df.time):\n",
    "    if type(time) != str:\n",
    "        indices_todrop.append(num)\n",
    "        \n",
    "df.drop(indices_todrop, inplace = True)\n",
    "\n",
    "df = df.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtro por el dia 24 o 25\n",
    "\n",
    "FECHA = 25\n",
    "\n",
    "df = df[df.day == FECHA]\n",
    "df = df.reset_index(drop = True)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Eliminio las filas que no tengan texto en el tweet\n",
    "\n",
    "tweet_na = []\n",
    "for num, tweet in enumerate(df.tweet):\n",
    "    if type(tweet) != str:\n",
    "        tweet_na.append(num)\n",
    "df.drop(tweet_na, inplace = True)\n",
    "df = df.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_csv(\"tweets_24_notendencias_preprocesado.csv\", sep = \";\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TERMINA PREPROCESAMIENTO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LISTA DE PALABRAS Y HASHTAGS TENDENCIAS (PARA FILTRAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargo las tendencias de ese dia\n",
    "\n",
    "tendencias = []\n",
    "with open(\"C:/Users/Daniel/Desktop/csv/dia 25/trends/dia 25 tendencias.txt\", \"r\", encoding = \"UTF-8\") as f:\n",
    "    tendencias.extend(f.readlines())\n",
    "    \n",
    "tendencias = [t[:-1].strip(\"\\t\") for num, t in enumerate(tendencias) if num != len(tendencias) - 1]\n",
    "\n",
    "df_tendencias = pd.DataFrame(tendencias, columns = [\"trends\"])\n",
    "df_tendencias = df_tendencias.trends.unique()\n",
    "df_tendencias = pd.DataFrame(df_tendencias, columns = [\"trends\"])\n",
    "solo_tendencias = list(df_tendencias.trends.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista de palabras tendencias y hashtags tendencias\n",
    "\n",
    "hashtags_tendencias = [t for t in solo_tendencias if t[0] == \"#\"]\n",
    "hashtags_tendencias_sin_numeral = [t.strip(\"#\").lower() for t in solo_tendencias if t[0] == \"#\"]\n",
    "\n",
    "palabras_tendencias = [t.strip(\"\\t\") for t in solo_tendencias if t[0] != \"#\"]\n",
    "palabras_tendencias_lower = [t.strip(\"\\t\").lower() for t in solo_tendencias if t[0] != \"#\"]\n",
    "\n",
    "print(\"hashtags_tendencias:\", len(hashtags_tendencias))\n",
    "print(\"hashtags_tendencias_sin_numeral:\", len(hashtags_tendencias_sin_numeral))\n",
    "\n",
    "print(\"palabras_tendencias:\", len(palabras_tendencias))\n",
    "print(\"palabras_tendencias_lower:\", len(palabras_tendencias_lower))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FUNCIONES ESPECIALES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_hashtags_no_tendencias(df_aux):\n",
    "    \"\"\"Retorna un diccionario con los hashtags no tendencias que mas se repiten\"\"\"\n",
    "    # Cuento cuantos hashtags hay en el df y me quedo con los mas repetidos\n",
    "    hashtags_no_tendencias = list()\n",
    "    for h in df_aux.hashtags:\n",
    "        for hashtag in h.split(\",\"):\n",
    "            if hashtag not in hashtags_tendencias and hashtag != \"\":\n",
    "                hashtags_no_tendencias.append(hashtag)\n",
    "\n",
    "    hashtags_no_tendencias = Counter(hashtags_no_tendencias).most_common()\n",
    "    hashtags_no_tendencias = {h[0] : h[1] for h in hashtags_no_tendencias}\n",
    "\n",
    "    #print(\"Numero de hashtasg no tendencia:\", len(hashtags_no_tendencias))\n",
    "\n",
    "    return hashtags_no_tendencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elimina_hashtags_tendencias(df_aux, hashtags_tendencias_sin_numeral):\n",
    "    \"\"\"Elimina las filas que tengan hashtags tendencias\"\"\"\n",
    "    # Saco los indices de las filas que tengan hashtags tendencias\n",
    "\n",
    "    hashtags_indices = list()\n",
    "    for num, h in enumerate(df_aux.hashtags):\n",
    "        for hashtag in h.split(\",\"):\n",
    "            if hashtag.lower() in hashtags_tendencias_sin_numeral:\n",
    "                hashtags_indices.append(num)\n",
    "\n",
    "    #print(\"Cantidad de tweets con hashtags tendencias:\", len(hashtags_indices))\n",
    "\n",
    "    df_aux.drop(hashtags_indices, inplace = True)\n",
    "\n",
    "    df_aux = df_aux.reset_index(drop = True)\n",
    "    return df_aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elimina_palabras_tendencias(df_aux, palabras_tendencias_lower):\n",
    "    \"\"\"Elimina las filas que tengan palabras tendencias\"\"\"\n",
    "    # Voy a quitar los tweets que tengan palabras claves tendencias\n",
    "\n",
    "    palabras_indices = list()\n",
    "    for num, tweet in enumerate(df_aux.tweet):\n",
    "        for palabra in palabras_tendencias_lower:\n",
    "            if tweet.lower().find(palabra) != -1:\n",
    "                palabras_indices.append(num)\n",
    "\n",
    "    #print(len(palabras_indices))\n",
    "\n",
    "    df_aux.drop(palabras_indices, inplace = True)\n",
    "    \n",
    "    df_aux = df_aux.reset_index(drop = True)\n",
    "    return df_aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def limpieza(df_aux):\n",
    "    \"\"\"Realiza toda las limpieza de texto\"\"\"\n",
    "    # Ahora voy a limpiar los tweets, para poder ver que palabras claves no tendencia se repiten mas\n",
    "\n",
    "    df_aux.tweet = df_aux.tweet.apply(normalize)\n",
    "    df_aux.tweet = df_aux.tweet.apply(deEmojify)\n",
    "    df_aux.tweet = df_aux.tweet.apply(cleanTxt)\n",
    "    df_aux.tweet = df_aux.tweet.apply(replace_punct)\n",
    "    df_aux.tweet = df_aux.tweet.apply(replace_num)\n",
    "    df_aux.tweet = df_aux.tweet.apply(quita_palabras_pequeñas)\n",
    "    df_aux.tweet = df_aux.tweet.apply(tokenizador)\n",
    "    df_aux.tweet = df_aux.tweet.apply(foo)\n",
    "    return df_aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elimina_tweets_vacios(df_aux):\n",
    "    \"\"\"ELimina los tweets que no tengan texto despues de aplicar las limpiezas\"\"\"\n",
    "    # Dropeo las filas de tweets que tengan texto \"\"\n",
    "\n",
    "    tweet_vacios = []\n",
    "\n",
    "    for num, tweet in enumerate(df_aux.tweet):\n",
    "        if tweet == \"\":\n",
    "            tweet_vacios.append(num)\n",
    "\n",
    "    #print(len(tweet_vacios))        \n",
    "\n",
    "    df_aux.drop(tweet_vacios, inplace = True)\n",
    "\n",
    "    df_aux = df_aux.reset_index(drop = True)\n",
    "\n",
    "    return df_aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_palabras_no_tendencias(df_aux):\n",
    "    \"\"\"Retorna un diccionario con las palabras no tendencia que mas se repiten\"\"\"\n",
    "\n",
    "    # Cuanto cuantos palabras hay en el df y me quedo con los mas repetidos\n",
    "\n",
    "    palabras_no_tendencias = list()\n",
    "    for p in df_aux.tweet:\n",
    "        for palabra in p.split(\" \"):\n",
    "            palabras_no_tendencias.append(palabra)\n",
    "\n",
    "    palabras_no_tendencias = Counter(palabras_no_tendencias).most_common()\n",
    "    palabras_no_tendencias = {h[0] : h[1] for h in palabras_no_tendencias}\n",
    "\n",
    "    #print(len(palabras_no_tendencias))\n",
    "    return palabras_no_tendencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_h(df_aux, hashtags_no_tendencias):\n",
    "    \"\"\"Retorna un dataframe de solo hashtags, con una columna donde aparece la no tendencia\"\"\"\n",
    "    df_h = df_aux[df_aux.hashtags != \"\"]\n",
    "\n",
    "    df_h = df_h.reset_index(drop = True)\n",
    "\n",
    "    df_h[\"trends\"] = [[h if h in hashtags_no_tendencias else 0 for h in hashtag.split(\",\")] for hashtag in df_h.hashtags]\n",
    "\n",
    "    df_h.trends = df_h.trends.apply(lambda x : [h for h in x if h != 0])\n",
    "\n",
    "    indices_drop = list()\n",
    "    for num, t in enumerate(df_h.trends):\n",
    "        if t == []:\n",
    "            indices_drop.append(num)\n",
    "\n",
    "    df_h.drop(indices_drop, inplace = True)\n",
    "\n",
    "    df_h = df_h.reset_index(drop = True)\n",
    "\n",
    "\n",
    "    indices_para_clonar = list()\n",
    "    for num, t in enumerate(df_h.trends):\n",
    "        if len(t) > 1:\n",
    "            indices_para_clonar.append(num)\n",
    "\n",
    "\n",
    "    dic_indices = {indice : [len(trends), trends] for indice, trends in zip(indices_para_clonar, df_h.loc[indices_para_clonar].trends)}\n",
    "\n",
    "    df_v = pd.DataFrame(columns = df_h.columns)\n",
    "\n",
    "    for key in dic_indices.keys():\n",
    "        for time in range(dic_indices[key][0]):\n",
    "            df_d = pd.DataFrame(df_h.loc[key]).T\n",
    "            df_d.drop(df_d.columns[-1], axis = 1, inplace = True)\n",
    "            df_d[\"trends\"] = dic_indices[key][1][time]\n",
    "            df_v = pd.concat([df_v, df_d])\n",
    "\n",
    "\n",
    "\n",
    "    df_h.drop(indices_para_clonar, inplace = True)\n",
    "\n",
    "    df_h = df_h.reset_index(drop = True)\n",
    "\n",
    "    df_h.trends = df_h.trends.apply(lambda x : x[0]) \n",
    "\n",
    "    df_h = pd.concat([df_h, df_v])\n",
    "    df_h.trends = df_h.trends.apply(lambda x : \"#\" + x)\n",
    "\n",
    "    #df_h.to_csv(\"H_6.csv\", sep = \";\", index = False)\n",
    "\n",
    "    return df_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_p(df_aux, palabras_no_tendencias):\n",
    "    \"\"\"Retorna un dataframe de solo palabras, con una columna donde aparece la no tendencia\"\"\"\n",
    "    df_p = df_aux[df_aux.hashtags == \"\"]\n",
    "    df_p = df_p.reset_index(drop = True)\n",
    "\n",
    "    df_p[\"trends\"] = [[p for p in palabra.split(\" \") if p in palabras_no_tendencias] for palabra in df_p.tweet]\n",
    "\n",
    "    indices_drop = list()\n",
    "    for num, trend in enumerate(df_p.trends):\n",
    "        if trend == []:\n",
    "            indices_drop.append(num)\n",
    "\n",
    "    df_p.drop(indices_drop, inplace = True)\n",
    "    df_p = df_p.reset_index(drop = True)\n",
    "\n",
    "\n",
    "    indices_multi = []\n",
    "    for num, t in enumerate(df_p.trends):\n",
    "        if len(t) >= 2:\n",
    "            indices_multi.append(num)\n",
    "\n",
    "\n",
    "    df_dup = df_p.iloc[indices_multi, :]\n",
    "    df_dup = df_dup.reset_index(drop = True)\n",
    "\n",
    "\n",
    "    indices_dup = df_dup.index.tolist()\n",
    "    dic_indices = {indice : [len(trends), trends] for indice, trends in zip(indices_dup, df_dup.trends)}\n",
    "\n",
    "    vacio = list()\n",
    "    for key, value in dic_indices.items():\n",
    "        prueba = np.tile([list(df_dup.iloc[key])], (value[0], 1))\n",
    "        vacio.extend(prueba)\n",
    "\n",
    "    df_multi = pd.DataFrame(vacio, columns = df_dup.columns)\n",
    "\n",
    "\n",
    "    palabras = list()\n",
    "    for i in range(len(df_dup.trends)):\n",
    "        words = df_dup.trends[i]\n",
    "        for j in range(len(words)):\n",
    "            word = words[j]\n",
    "            palabras.append(word)\n",
    "\n",
    "    df_multi['trends'] = palabras\n",
    "\n",
    "\n",
    "    df_uni = df_p[~(df_p.index.isin(indices_multi))]\n",
    "    df_uni.trends = df_uni.trends.apply(lambda x : x[0])\n",
    "\n",
    "    df_palabras = pd.concat([df_multi, df_uni])\n",
    "\n",
    "    return df_palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_no_trends(df_h, df_p, start, num):\n",
    "    \"\"\"Retorna la concatenacion de los otros dos dataframes, ademas de una lista con la hora\n",
    "       donde esa palabra o hashtag no tendencia se repite mas\"\"\"\n",
    "    df_concat = pd.concat([df_h, df_p])\n",
    "    \n",
    "    df_summary = pd.DataFrame(df_concat.trends.value_counts()).reset_index()\n",
    "    df_summary.columns = [\"trend\", \"total_tweet\"]\n",
    "    \n",
    "    df_summary[\"total_interaction\"] = [df_concat[df_concat.trends == trend].interaccion.sum() for trend in df_summary.trend]\n",
    "    \n",
    "    df_summary = df_summary.sort_values(\"total_interaction\", ascending = False).iloc[: num, :]\n",
    "    \n",
    "    no_trends = df_summary.trend.tolist()\n",
    "    \n",
    "    return df_concat, [[nt, start] for nt in no_trends]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TERMINA FUNCIONES ESPECIALES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BUCLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "to_df = list() # Lista que guarda la palabra o hashtag no tendencia en la hora que mas se repite\n",
    "df_target = pd.DataFrame(columns = df.columns) #Dataframe con todos los dataframes concatenados (palabras y hashtags)\n",
    "\n",
    "starts = [i for i in range(24)]\n",
    "for start in starts:\n",
    "    \n",
    "    num = df_summary[df_summary.hour == start].shape[0] #Numero de no tendencia que va a coleccionar\n",
    "    \n",
    "    df_aux = df[df.hour == start] #Filtro por hora\n",
    "    df_aux = df_aux.reset_index(drop = True)\n",
    "     \n",
    "    # Limpieza\n",
    "    hashtags_no_tendencias = f_hashtags_no_tendencias(df_aux)\n",
    "    df_aux                 = elimina_hashtags_tendencias(df_aux, hashtags_tendencias_sin_numeral)\n",
    "    df_aux                 = elimina_palabras_tendencias(df_aux, palabras_tendencias_lower)\n",
    "    df_aux                 = limpieza(df_aux)\n",
    "    df_aux                 = elimina_tweets_vacios(df_aux)\n",
    "    palabras_no_tendencias = f_palabras_no_tendencias(df_aux)\n",
    "    \n",
    "    df_aux.hashtags = df_aux.hashtags.apply(str)\n",
    "    \n",
    "    df_h = get_df_h(df_aux, hashtags_no_tendencias)\n",
    "    df_p = get_df_p(df_aux, palabras_no_tendencias)\n",
    "    \n",
    "    df_concat, no_trends = get_df_no_trends(df_h, df_p, start, num)\n",
    "    \n",
    "    df_target = pd.concat([df_target, df_concat])\n",
    "    to_df.extend(no_trends)\n",
    "    \n",
    "    print(start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_target.to_csv(\"tweets_24_notedencias_preprocesado_labels.csv\", sep = \";\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_st = pd.DataFrame(to_df, columns = [\"trend\", \"start_lifetime\"])\n",
    "#df_st.to_csv(\"tweets_24_start_lifetime_notendencias.csv\", sep = \";\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
